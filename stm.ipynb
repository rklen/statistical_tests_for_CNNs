{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import scipy\n",
    "import statsmodels\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from scipy.stats import ttest_rel, wilcoxon, shapiro, bartlett, levene\n",
    "from matplotlib.collections import LineCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "2.7.0\n",
      "1.7.3\n",
      "0.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(scipy.__version__)\n",
    "print(statsmodels.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data for binary classification\n",
    "img_height = 128\n",
    "filenames = os.listdir('C:/Users/Oona/Documents/Tpc/thr/covid/normal/')\n",
    "filenames1 = os.listdir('C:/Users/Oona/Documents/Tpc/thr/covid/cvd')\n",
    "imgs=[]\n",
    "for i in range(3000):\n",
    "    img_path='C:/Users/Oona/Documents/Tpc/thr/covid/normal/{}'.format(filenames[i])\n",
    "    img=cv2.imread(img_path)\n",
    "    img=cv2.resize(img[:,:,0],(img_height,img_height))\n",
    "    imgs.append(img)\n",
    "    img_path1='C:/Users/Oona/Documents/Tpc/thr/covid/cvd/{}'.format(filenames1[i])\n",
    "    img1=cv2.imread(img_path1)\n",
    "    img1=cv2.resize(img1[:,:,0],(img_height,img_height))\n",
    "    imgs.append(img1)\n",
    "x_train=np.array(imgs[0:2400])\n",
    "y_train=np.array([0,1]*1200)\n",
    "x_test=np.array(imgs[2400:3000])\n",
    "y_test=np.array([0,1]*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data for multi-class classification\n",
    "img_height = 128\n",
    "filenames0 = os.listdir('C:/Users/Oona/Documents/Tpc/thr/covid/normal/')\n",
    "filenames1 = os.listdir('C:/Users/Oona/Documents/Tpc/thr/covid/cvd')\n",
    "filenames2 = os.listdir('C:/Users/Oona/Documents/Tpc/thr/pneumonia/ill')\n",
    "filenames3 = os.listdir('C:/Users/Oona/Documents/Tpc/thr/tuberculosis/tb')\n",
    "imgs=[]\n",
    "for i in range(700):\n",
    "    img_path='C:/Users/Oona/Documents/Tpc/thr/covid/normal/{}'.format(filenames[i])\n",
    "    img=cv2.imread(img_path)\n",
    "    img=cv2.resize(img[:,:,0],(img_height,img_height))\n",
    "    imgs.append(img)\n",
    "    img_path1='C:/Users/Oona/Documents/Tpc/thr/covid/cvd/{}'.format(filenames1[i])\n",
    "    img1=cv2.imread(img_path1)\n",
    "    img1=cv2.resize(img1[:,:,0],(img_height,img_height))\n",
    "    imgs.append(img1)\n",
    "    img_path2='C:/Users/Oona/Documents/Tpc/thr/pneumonia/ill/{}'.format(filenames2[i])\n",
    "    img2=cv2.imread(img_path2)\n",
    "    img2=cv2.resize(img2[:,:,0],(img_height,img_height))\n",
    "    imgs.append(img2)\n",
    "    img_path3='C:/Users/Oona/Documents/Tpc//thr/tuberculosis/tb/{}'.format(filenames3[i])\n",
    "    img3=cv2.imread(img_path3)\n",
    "    img3=cv2.resize(img3[:,:,0],(img_height,img_height))\n",
    "    imgs.append(img3)\n",
    "\n",
    "def divideImgsIntoSets(imgs,k):\n",
    "\n",
    "    numberOfImgs=len(imgs)\n",
    "    testSetSize=int(numberOfImgs/5)\n",
    "    x_train=np.array(imgs[0:(k*testSetSize)]+imgs[((k+1)*testSetSize):numberOfImgs])\n",
    "    x_test=np.array(imgs[(k*testSetSize):((k+1)*testSetSize)])\n",
    "    return([x_train,x_test])\n",
    "\n",
    "y_train=np.array([np.array([1,0,0,0]),np.array([0,1,0,0]),np.array([0,0,1,0]),np.array([0,0,0,1])]*560)\n",
    "y_test=np.array([np.array([1,0,0,0]),np.array([0,1,0,0]),np.array([0,0,1,0]),np.array([0,0,0,1])]*140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n",
      "962\n",
      "10.808988764044944\n",
      "6.063107084385669\n",
      "[ 3  9 11 14  8  9 15 12  5 10 13  8 11 10  4 13  4  4  6  4  6  6  5 12\n",
      " 12  6 18 18  6 19  7 12 13 22 10  5 20  5 12 25  5  7  2  9  6 18 24 11\n",
      " 11 13 21  7  6  9 13  5  2  1  7  3 18 15  6 20 26 16 13 10 11  6  3  4\n",
      " 15  6 26  6 10 10 20  5 17 13 18 13 14 11  4 11 23]\n",
      "191\n",
      "0.19854469854469856\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "#data for binary segmentation\n",
    "posPatients = np.concatenate((np.array([2,4,5,7,9,11,12,13,16,17,18,19,21,22]),range(23,52),np.array([55]),\n",
    "                                range(60,63),range(64,74),range(75,82),range(83,104),range(105,109)))\n",
    "locList = []\n",
    "maskList = []\n",
    "posList = []\n",
    "img_height=128\n",
    "limit=0.25\n",
    "pixLimit=5\n",
    "\n",
    "for i in range(len(posPatients)):\n",
    "    if posPatients[i]<23:\n",
    "        if posPatients[i]==12:\n",
    "            img_path = 'C:/Users/Oona/Documents/Tpc/hnc/anom_data/positiiviset/12_anon/12_maski/12_maskit.img'\n",
    "        else:\n",
    "            img_path = 'C:/Users/Oona/Documents/Tpc/hnc/anom_data/positiiviset/{}_anon/{}_maski/{}_maski.img'.format(\n",
    "                        posPatients[i],posPatients[i],posPatients[i])\n",
    "    else:\n",
    "        if posPatients[i]==23:\n",
    "            img_path = 'C:/Users/Oona/Documents/Tpc/hnc/anom_data/positiiviset/23/23_maski/23_nifti_maski_edit.img'\n",
    "        if posPatients[i]==24:\n",
    "            img_path = 'C:/Users/Oona/Documents/Tpc/hnc/anom_data/positiiviset/24/24_maski/24_maski.img'\n",
    "        if posPatients[i]==38:\n",
    "            img_path = 'C:/Users/Oona/Documents/Tpc/hnc/anom_data/positiiviset/38/38_nifti_maski/38_nifti_maski_edit.img'\n",
    "        if posPatients[i] not in [23,24,38]:    \n",
    "            img_path = 'C:/Users/Oona/Documents/Tpc/hnc/anom_data/positiiviset/{}/{}_nifti_maski/{}_nifti_maski.img'.format(\n",
    "                        posPatients[i],posPatients[i],posPatients[i])\n",
    "    img = nib.load(img_path)\n",
    "    img_data = img.get_fdata()\n",
    "    locations = []\n",
    "    masks = []\n",
    "    for j in range(img_data.shape[2]):\n",
    "        slice = np.array(img_data[:,:,j])\n",
    "        if np.max(slice)>0:\n",
    "            slice=cv2.resize(slice,(img_height,img_height))\n",
    "            for k in range(img_height):\n",
    "                for l in range(img_height):\n",
    "                    if slice[k,l]>limit:\n",
    "                        slice[k,l]=1\n",
    "                    else:\n",
    "                        slice[k,l]=0\n",
    "            if np.sum(slice)>pixLimit:\n",
    "                locations.append(j)\n",
    "                masks.append(slice)\n",
    "    locList.append(np.array(locations))\n",
    "    maskList.append(masks)\n",
    "\n",
    "for i in range(len(posPatients)):\n",
    "    if posPatients[i]<23:\n",
    "        img_path = 'C:/Users/Oona/Documents/Tpc/hnc/anom_data/positiiviset/{}_anon/{}_nifti_pet/{}_nifti_pet.img'.format(\n",
    "                        posPatients[i],posPatients[i],posPatients[i])\n",
    "    else:\n",
    "        img_path = 'C:/Users/Oona/Documents/Tpc/hnc/anom_data/positiiviset/{}/{}_nifti_pet/{}_nifti_pet.img'.format(\n",
    "                        posPatients[i],posPatients[i],posPatients[i])\n",
    "    img = nib.load(img_path)\n",
    "    img_data = img.get_fdata()\n",
    "    pos = []\n",
    "    for j in range(len(locList[i])):\n",
    "        pos.append(cv2.resize(np.array(img_data[:,:,locList[i][j]]),(img_height,img_height)))\n",
    "    posList.append(pos)\n",
    "\n",
    "def convert(img):\n",
    "\n",
    "    img=(img-np.min(img))/(np.max(img)-np.min(img))*255\n",
    "    for i in range(img_height):\n",
    "        for j in range(img_height):\n",
    "            img[i,j]=int(img[i,j])\n",
    "    return img.astype('int64')\n",
    "\n",
    "for i in range(len(posList)):\n",
    "    for j in range(len(posList[i])):\n",
    "        posList[i][j]=convert(posList[i][j])\n",
    "\n",
    "numberOfSlices=[]\n",
    "finPatients=0\n",
    "for i in range(len(posList)):\n",
    "    numberOfSlices.append(len(posList[i]))\n",
    "    if len(posList[i])>0:\n",
    "        finPatients+=1\n",
    "numberOfSlices=np.array(numberOfSlices)\n",
    "print(finPatients)\n",
    "print(sum(numberOfSlices))\n",
    "print(np.mean(numberOfSlices))\n",
    "print(np.std(numberOfSlices))\n",
    "print(numberOfSlices)\n",
    "\n",
    "testSetPatients=np.concatenate((range(0,70,5),np.array([72,82,83])))\n",
    "\n",
    "numTestSetSlices=0\n",
    "numTestSetPatients=0\n",
    "for i in range(len(posList)):\n",
    "    if i in testSetPatients:\n",
    "        numTestSetSlices+=len(posList[i])\n",
    "        if len(posList[i])>0:\n",
    "            numTestSetPatients+=1\n",
    "\n",
    "print(numTestSetSlices)\n",
    "print(numTestSetSlices/sum(numberOfSlices))\n",
    "print(numTestSetPatients)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(len(posList)):\n",
    "    if i in testSetPatients:\n",
    "        for j in range(len(posList[i])):\n",
    "            img=(posList[i][j]-np.min(posList[i][j]))/(np.max(posList[i][j])-np.min(posList[i][j]))\n",
    "            x_test.append(img)\n",
    "            y_test.append(cv2.resize(maskList[i][j],(img_height,img_height)))   \n",
    "    else:\n",
    "        for j in range(len(posList[i])):\n",
    "            img=(posList[i][j]-np.min(posList[i][j]))/(np.max(posList[i][j])-np.min(posList[i][j]))\n",
    "            x_train.append(img)\n",
    "            y_train.append(cv2.resize(maskList[i][j],(img_height,img_height)))  \n",
    "\n",
    "x_train=np.array(x_train)\n",
    "y_train=np.array(y_train)\n",
    "x_test=np.array(x_test)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convolutional neural networks\n",
    "\n",
    "def unetForBinClass(x_train,y_train,x_test,numberOfEpochs):\n",
    "\n",
    "    img_height=x_train[0].shape[0]\n",
    "    model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(16, 3, activation='relu', input_shape=(img_height,img_height,1)),\n",
    "                                    tf.keras.layers.Conv2D(16, 3, activation='relu'),\n",
    "                                    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "                                    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "                                    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "                                    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "                                    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "                                    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "                                    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "                                    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "                                    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "                                    tf.keras.layers.MaxPooling2D(strides=(2, 2)),\n",
    "                                    tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(128, activation='relu'),\n",
    "                                    tf.keras.layers.Dense(64, activation='relu'),\n",
    "                                    tf.keras.layers.Dense(32, activation='relu'),\n",
    "                                    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ] \n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(1e-3),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "    model.fit(x=x_train,y=y_train,epochs=numberOfEpochs,validation_split=0.3,shuffle=True)\n",
    "    predictions=model.predict(x_test)\n",
    "    trainPreds=model.predict(x_train)\n",
    "    return([predictions,trainPreds])\n",
    "\n",
    "def inceptionForBinClass(x_train,y_train,x_test,numEpochs):\n",
    "    \n",
    "    img_height=x_train[0].shape[0]\n",
    "    if len(x_train.shape)==3:\n",
    "        x_train=np.rollaxis(np.array([x_train,x_train,x_train]),0,4)\n",
    "        x_test=np.rollaxis(np.array([x_test,x_test,x_test]),0,4)\n",
    "    ntf_model = keras.applications.InceptionV3(weights=None,input_shape=(img_height,img_height,3),include_top=False)\n",
    "    ntf_model.trainable = True\n",
    "    inputs = keras.Input(shape=(img_height,img_height,3))\n",
    "    x = ntf_model(inputs, training=True)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = keras.layers.Dense(1)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=keras.optimizers.SGD(1e-3),\n",
    "                loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=[keras.metrics.BinaryAccuracy()])\n",
    "    model.fit(x=x_train, y=y_train, epochs=numEpochs, validation_split=0.3, shuffle=True)\n",
    "    ntfPredictions=model.predict(x_test)\n",
    "    ntfTrainPreds=model.predict(x_train)\n",
    "    return([ntfPredictions, ntfTrainPreds])\n",
    "\n",
    "def unetForMultiClass(x_train,y_train,x_test,numberOfEpochs):\n",
    "\n",
    "    img_height=x_train[0].shape[0]\n",
    "    model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(16, 3, activation='relu', input_shape=(img_height,img_height,1)),\n",
    "                                    tf.keras.layers.Conv2D(16, 3, activation='relu'),\n",
    "                                    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "                                    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "                                    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "                                    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "                                    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "                                    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "                                    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "                                    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "                                    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "                                    tf.keras.layers.MaxPooling2D(strides=(2, 2)),\n",
    "                                    tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(128, activation='relu'),\n",
    "                                    tf.keras.layers.Dense(64, activation='relu'),\n",
    "                                    tf.keras.layers.Dense(32, activation='relu'),\n",
    "                                    tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ] \n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(1e-3),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.CategoricalAccuracy()]\n",
    "    )\n",
    "    model.fit(x=x_train,y=y_train,epochs=numberOfEpochs,validation_split=0.3,shuffle=True)\n",
    "    predictions=model.predict(x_test)\n",
    "    trainPreds=model.predict(x_train)\n",
    "    return([predictions,trainPreds])\n",
    "\n",
    "def inceptionForMultiClass(x_train,y_train,x_test,numEpochs):\n",
    "    \n",
    "    img_height=x_train[0].shape[0]\n",
    "    if len(x_train.shape)==3:\n",
    "        x_train=np.rollaxis(np.array([x_train,x_train,x_train]),0,4)\n",
    "        x_test=np.rollaxis(np.array([x_test,x_test,x_test]),0,4)\n",
    "    ntf_model = keras.applications.InceptionV3(weights=None,input_shape=(img_height,img_height,3),include_top=False)\n",
    "    ntf_model.trainable = True\n",
    "    inputs = keras.Input(shape=(img_height,img_height,3))\n",
    "    x = ntf_model(inputs, training=True)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = keras.layers.Dense(4,activation='softmax')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=keras.optimizers.SGD(1e-3),\n",
    "                loss=keras.losses.CategoricalCrossentropy(),\n",
    "                metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "    model.fit(x=x_train, y=y_train, epochs=numEpochs, validation_split=0.3, shuffle=True)\n",
    "    ntfPredictions=model.predict(x_test)\n",
    "    ntfTrainPreds=model.predict(x_train)\n",
    "    return([ntfPredictions, ntfTrainPreds])\n",
    "\n",
    "def unetForSegmentation(x_train,y_train,x_test,numEpochs):\n",
    "\n",
    "    img_height=x_train[0].shape[0]\n",
    "\n",
    "    #U-Net model\n",
    "    inputs = tf.keras.layers.Input(shape=(img_height,img_height,1))\n",
    "\n",
    "    #Contraction path\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "\n",
    "    #Expansive path\n",
    "    u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c3)\n",
    "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "\n",
    "    u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(1, (1,1), activation='linear')(c9) #sigmoid or linear\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.fit(x=x_train,y=y_train,epochs=numEpochs,validation_split=0.3,shuffle=True)\n",
    "    trainPreds=model.predict(x_train)\n",
    "    predictions=model.predict(x_test)\n",
    "    return([predictions,trainPreds])\n",
    "\n",
    "def deeperUnetForSegmentation(x_train,y_train,x_test,numEpochs):\n",
    "\n",
    "    img_height=x_train[0].shape[0]\n",
    "\n",
    "    #U-Net model\n",
    "    inputs = tf.keras.layers.Input(shape=(img_height,img_height,1))\n",
    "\n",
    "    #Contraction path\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "\n",
    "    #Expansive path\n",
    "\n",
    "    u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c4)\n",
    "    u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = tf.keras.layers.Dropout(0.1)(c7)\n",
    "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "\n",
    "    u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "\n",
    "    u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(1, (1,1), activation='linear')(c9) #sigmoid or linear\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.fit(x=x_train,y=y_train,epochs=numEpochs,validation_split=0.3,shuffle=True)\n",
    "    trainPreds=model.predict(x_train)\n",
    "    predictions=model.predict(x_test)\n",
    "    return([predictions,trainPreds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions,trainPreds=unetForBinClass(x_train,y_train,x_test,10)\n",
    "np.savetxt('unet_binClass_trainPreds.txt',trainPreds)\n",
    "np.savetxt('unet_binClass_predictions.txt',predictions)\n",
    "predictions,trainPreds=inceptionForBinClass(x_train,y_train,x_test,10)\n",
    "np.savetxt('inception_binClass_trainPreds.txt',trainPreds)\n",
    "np.savetxt('inception_binClass_predictions.txt',predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(5):\n",
    "    x_train,x_test=divideImgsIntoSets(imgs,k)\n",
    "    for j in range(5):\n",
    "        predictions,trainPreds=unetForMultiClass(x_train,y_train,x_test,10)\n",
    "        np.savetxt('unet_multiClass_predictions_{}_{}.txt'.format(k,j),predictions)\n",
    "        predictions,trainPreds=inceptionForMultiClass(x_train,y_train,x_test,10)\n",
    "        np.savetxt('inception_multiClass_predictions_{}_{}.txt'.format(k,j),predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions,trainPreds=unetForSegmentation(x_train,y_train,x_test,50)\n",
    "for i in range(len(trainPreds)):\n",
    "    np.savetxt('unet_segmentation_trainPreds_{}.txt'.format(i), trainPreds[i][:,:,0])\n",
    "for i in range(len(predictions)):\n",
    "    np.savetxt('unet_segmentation_predictions_{}.txt'.format(i), predictions[i][:,:,0])\n",
    "predictions,trainPreds=deeperUnetForSegmentation(x_train,y_train,x_test,50)\n",
    "for i in range(len(trainPreds)):\n",
    "    np.savetxt('deep_segmentation_trainPreds_{}.txt'.format(i), trainPreds[i][:,:,0])\n",
    "for i in range(len(predictions)):\n",
    "    np.savetxt('deep_segmentation_predictions_{}.txt'.format(i), predictions[i][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for evaluation metrics\n",
    "\n",
    "def evaluateBinClassPredictions(predictions,y_test,trainPreds,y_train):\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, trainPreds, drop_intermediate=False)\n",
    "    youden = thresholds[np.argmax(tpr - fpr)]\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if predictions[i]<youden:\n",
    "            if y_test[i]==0:\n",
    "                TN+=1\n",
    "            else:\n",
    "                FN+=1\n",
    "        else:\n",
    "            if y_test[i]==1:\n",
    "                TP+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "    acc = (TN+TP)/(TN+FN+TP+FP)\n",
    "    sen = TP/(FN+TP)\n",
    "    spe = TN/(TN+FP)\n",
    "    pre = TP/(FP+TP)\n",
    "    youden = sen+spe-1\n",
    "    f1 = 2*pre*sen/(pre+sen)\n",
    "    p_e=((TP+FN)*(TP+FP)+(TN+FP)*(TN+FN))/(TP+TN+FN+FP)**2\n",
    "    kappa=(acc-p_e)/(1-p_e)\n",
    "    mcc=(TN*TP-FN*FP)/((TP+FN)*(TP+FP)*(TN+FP)*(TN+FN))**0.5\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predictions, drop_intermediate=False)\n",
    "    auc1 = auc(fpr,tpr)\n",
    "    return([acc,sen,spe,pre,youden,f1,kappa,mcc,auc1])\n",
    "\n",
    "def tablesForMcNemar(predictions,trainPreds,predictions1,trainPreds1,y_test,y_train):\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, trainPreds, drop_intermediate=False)\n",
    "    youden = thresholds[np.argmax(tpr - fpr)]\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, trainPreds1, drop_intermediate=False)\n",
    "    youden1 = thresholds[np.argmax(tpr - fpr)]\n",
    "    negTable=np.zeros((2,2))\n",
    "    posTable=np.zeros((2,2))\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==1:\n",
    "            posTable[int(predictions[i]<youden),int(predictions1[i]<youden1)]+=1\n",
    "        else:\n",
    "            negTable[int(predictions[i]<youden),int(predictions1[i]<youden1)]+=1\n",
    "    return([posTable,negTable])\n",
    "\n",
    "def evaluateMultiClassPredictions(predictions,y_test):\n",
    "\n",
    "    n=len(predictions[0])\n",
    "    confMatrix=np.zeros((n,n))\n",
    "    for i in range(len(predictions)):\n",
    "        confMatrix[np.argmax(y_test[i]),np.argmax(predictions[i])]+=1\n",
    "    counts=np.zeros((n,4))\n",
    "    macroAverageMetrics=np.zeros((6))\n",
    "    p_e=0\n",
    "    c_0=0\n",
    "    c_1=0\n",
    "    for i in range(n):\n",
    "        TP=confMatrix[i,i]\n",
    "        FN=np.sum(confMatrix[i,:])-TP\n",
    "        FP=np.sum(confMatrix[:,i])-TP\n",
    "        TN=np.sum(confMatrix)-TP-FN-FP\n",
    "        counts[i,:]=[TP,FN,FP,TN]\n",
    "        acc = (TN+TP)/(TN+FN+TP+FP)\n",
    "        sen = TP/(FN+TP)\n",
    "        spe = TN/(TN+FP)\n",
    "        pre = TP/(FP+TP)\n",
    "        youden = sen+spe-1\n",
    "        f1 = 2*pre*sen/(pre+sen)\n",
    "        macroAverageMetrics+=[acc,sen,spe,pre,youden,f1]\n",
    "        p_e+=np.sum(confMatrix[i,:])*np.sum(confMatrix[:,i])/np.sum(confMatrix)**2\n",
    "        c_0+=np.sum(confMatrix[i,:])**2/np.sum(confMatrix)**2\n",
    "        c_1+=np.sum(confMatrix[:,i])**2/np.sum(confMatrix)**2\n",
    "    macroAverageMetrics=macroAverageMetrics/n\n",
    "    TP=np.sum(counts[:,0])\n",
    "    FN=np.sum(counts[:,1])\n",
    "    FP=np.sum(counts[:,2])\n",
    "    TN=np.sum(counts[:,3])\n",
    "    acc = (TN+TP)/(TN+FN+TP+FP)\n",
    "    sen = TP/(FN+TP)\n",
    "    spe = TN/(TN+FP)\n",
    "    pre = TP/(FP+TP)\n",
    "    youden = sen+spe-1\n",
    "    f1 = 2*pre*sen/(pre+sen)\n",
    "    microAverageMetrics=np.array([acc,sen,spe,pre,youden,f1])\n",
    "    p_0=TP/np.sum(confMatrix)\n",
    "    kappa=(p_0-p_e)/(1-p_e)\n",
    "    mcc=(p_0-p_e)/((1-c_0)*(1-c_1))**0.5\n",
    "    return([macroAverageMetrics,microAverageMetrics,kappa,mcc])\n",
    "\n",
    "def dicesForLimit(predictions,y_test,limit):\n",
    "\n",
    "    dices=np.zeros((len(y_test)))\n",
    "    ious=np.zeros((len(y_test)))\n",
    "    img_height=predictions[0].shape[0]\n",
    "    for k in range(len(y_test)):\n",
    "        TP=0\n",
    "        FN=0\n",
    "        FP=0\n",
    "        for i in range(img_height):\n",
    "            for j in range(img_height):\n",
    "                if y_test[k,i,j]>0.5 and predictions[k,i,j]>limit:\n",
    "                    TP+=1\n",
    "                if y_test[k,i,j]>0.5 and predictions[k,i,j]<limit:\n",
    "                    FN+=1\n",
    "                if y_test[k,i,j]<0.5 and predictions[k,i,j]>limit:\n",
    "                    FP+=1\n",
    "        if TP+FN+FP>0:\n",
    "            dices[k]=2*TP/(2*TP+FN+FP)\n",
    "            ious[k]=TP/(TP+FN+FP)\n",
    "    return([dices,ious])\n",
    "\n",
    "def turnIntoBinaryMask(img,lim):\n",
    "\n",
    "    img_height=img.shape[0]\n",
    "    binM=np.zeros((img_height,img_height))\n",
    "    for i in range(img_height):\n",
    "        for j in range(img_height):\n",
    "            if img[i][j]>lim:\n",
    "                binM[i][j]=1\n",
    "    return binM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7566666666666667, 0.87, 0.6433333333333333, 0.7092391304347826, 0.5133333333333332, 0.781437125748503, 0.5133333333333334, 0.5270512152611412, 0.8453888888888889]\n",
      "[0.7316666666666667, 0.7533333333333333, 0.71, 0.7220447284345048, 0.4633333333333334, 0.737357259380098, 0.4633333333333334, 0.46376896546319546, 0.8207666666666666]\n",
      "[[207.  54.]\n",
      " [ 19.  20.]]\n",
      "pvalue      5.0622659111780655e-05\n",
      "statistic   19.0\n",
      "[[ 63.  44.]\n",
      " [ 24. 169.]]\n",
      "pvalue      0.02052693371370707\n",
      "statistic   24.0\n"
     ]
    }
   ],
   "source": [
    "unetTrainPreds=np.loadtxt('unet_binClass_trainPreds.txt')\n",
    "unetPreds=np.loadtxt('unet_binClass_predictions.txt')\n",
    "incTrainPreds=np.loadtxt('inception_binClass_trainPreds.txt')\n",
    "incPreds=np.loadtxt('inception_binClass_predictions.txt')\n",
    "y_test=np.array([0,1]*int(unetPreds.shape[0]/2))\n",
    "y_train=np.array([0,1]*int(unetTrainPreds.shape[0]/2))\n",
    "print(evaluateBinClassPredictions(unetPreds,y_test,unetTrainPreds,y_train))\n",
    "print(evaluateBinClassPredictions(incPreds,y_test,incTrainPreds,y_train))\n",
    "posTable,negTable=tablesForMcNemar(unetPreds,unetTrainPreds,incPreds,incTrainPreds,y_test,y_train)\n",
    "print(posTable)\n",
    "print(mcnemar(posTable))\n",
    "print(negTable)\n",
    "print(mcnemar(negTable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85714286 0.71428571 0.9047619  0.75968604 0.61904762 0.69319037\n",
      " 0.85714286 0.71428571 0.9047619  0.71428571 0.61904762 0.71428571\n",
      " 0.61904762 0.64106434]\n",
      "[0.80089286 0.60178571 0.8672619  0.58915493 0.46904762 0.59066675\n",
      " 0.80089286 0.60178571 0.8672619  0.60178571 0.46904762 0.60178571\n",
      " 0.46904762 0.47252601]\n"
     ]
    }
   ],
   "source": [
    "array=np.zeros((25,14))\n",
    "array1=np.zeros((25,14))\n",
    "y_test=np.array([np.array([1,0,0,0]),np.array([0,1,0,0]),np.array([0,0,1,0]),np.array([0,0,0,1])]*140)\n",
    "for k in range(5):\n",
    "    for j in range(5):\n",
    "        predictions=np.loadtxt('unet_multiClass_predictions_{}_{}.txt'.format(k,j))\n",
    "        macroAverageMetrics,microAverageMetrics,kappa,mcc=evaluateMultiClassPredictions(predictions,y_test)\n",
    "        array[5*k+j,0:6]=macroAverageMetrics\n",
    "        array[5*k+j,6:12]=microAverageMetrics\n",
    "        array[5*k+j,12]=kappa\n",
    "        array[5*k+j,13]=mcc\n",
    "        predictions=np.loadtxt('inception_multiClass_predictions_{}_{}.txt'.format(k,j))\n",
    "        macroAverageMetrics,microAverageMetrics,kappa,mcc=evaluateMultiClassPredictions(predictions,y_test)\n",
    "        array1[5*k+j,0:6]=macroAverageMetrics\n",
    "        array1[5*k+j,6:12]=microAverageMetrics\n",
    "        array1[5*k+j,12]=kappa\n",
    "        array1[5*k+j,13]=mcc\n",
    "print(np.median(array,axis=0))\n",
    "print(np.median(array1,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.857\n",
      "0.801\n",
      "Ttest_relResult(statistic=5.221358637324015, pvalue=2.3749483100553514e-05)\n",
      "WilcoxonResult(statistic=25.0, pvalue=5.3882598876953125e-05)\n",
      "1\n",
      "0.714\n",
      "0.602\n",
      "Ttest_relResult(statistic=5.221358637324018, pvalue=2.3749483100553335e-05)\n",
      "WilcoxonResult(statistic=25.5, pvalue=6.365776062011719e-05)\n",
      "2\n",
      "0.905\n",
      "0.867\n",
      "Ttest_relResult(statistic=5.221358637324022, pvalue=2.3749483100553074e-05)\n",
      "WilcoxonResult(statistic=24.5, pvalue=5.3882598876953125e-05)\n",
      "3\n",
      "0.76\n",
      "0.589\n",
      "Ttest_relResult(statistic=7.999513823423033, pvalue=3.1593973183628113e-08)\n",
      "WilcoxonResult(statistic=4.0, pvalue=4.172325134277344e-07)\n",
      "4\n",
      "0.619\n",
      "0.469\n",
      "Ttest_relResult(statistic=5.221358637324019, pvalue=2.3749483100553294e-05)\n",
      "WilcoxonResult(statistic=25.5, pvalue=6.365776062011719e-05)\n",
      "5\n",
      "0.693\n",
      "0.591\n",
      "Ttest_relResult(statistic=3.9212486047391217, pvalue=0.0006427747665956361)\n",
      "WilcoxonResult(statistic=47.0, pvalue=0.0011548399925231934)\n",
      "6\n",
      "0.857\n",
      "0.801\n",
      "Ttest_relResult(statistic=5.221358637324019, pvalue=2.3749483100553294e-05)\n",
      "WilcoxonResult(statistic=25.0, pvalue=5.3882598876953125e-05)\n",
      "7\n",
      "0.714\n",
      "0.602\n",
      "Ttest_relResult(statistic=5.221358637324018, pvalue=2.3749483100553335e-05)\n",
      "WilcoxonResult(statistic=25.0, pvalue=5.3882598876953125e-05)\n",
      "8\n",
      "0.905\n",
      "0.867\n",
      "Ttest_relResult(statistic=5.221358637324022, pvalue=2.3749483100553074e-05)\n",
      "WilcoxonResult(statistic=24.5, pvalue=5.3882598876953125e-05)\n",
      "9\n",
      "0.714\n",
      "0.602\n",
      "Ttest_relResult(statistic=5.221358637324018, pvalue=2.3749483100553335e-05)\n",
      "WilcoxonResult(statistic=25.0, pvalue=5.3882598876953125e-05)\n",
      "10\n",
      "0.619\n",
      "0.469\n",
      "Ttest_relResult(statistic=5.221358637324022, pvalue=2.3749483100553074e-05)\n",
      "WilcoxonResult(statistic=24.5, pvalue=5.3882598876953125e-05)\n",
      "11\n",
      "0.714\n",
      "0.602\n",
      "Ttest_relResult(statistic=5.221358637324017, pvalue=2.374948310055342e-05)\n",
      "WilcoxonResult(statistic=25.0, pvalue=5.3882598876953125e-05)\n",
      "12\n",
      "0.619\n",
      "0.469\n",
      "Ttest_relResult(statistic=5.221358637324019, pvalue=2.3749483100553294e-05)\n",
      "WilcoxonResult(statistic=25.0, pvalue=5.3882598876953125e-05)\n",
      "13\n",
      "0.641\n",
      "0.473\n",
      "Ttest_relResult(statistic=6.193753275592556, pvalue=2.1200177924132553e-06)\n",
      "WilcoxonResult(statistic=18.0, pvalue=1.5079975128173828e-05)\n"
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    print(i)\n",
    "    print(round(np.median(array[:,i]),3))\n",
    "    print(round(np.median(array1[:,i]),3))\n",
    "    print(ttest_rel(array[:,i],array1[:,i]))\n",
    "    print(wilcoxon(array[:,i],array1[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[120.   7.   9.   4.]\n",
      " [ 15. 116.   3.   6.]\n",
      " [ 12.  13. 115.   0.]\n",
      " [  2.  96.   4.  38.]]\n",
      "[array([0.84732143, 0.69464286, 0.89821429, 0.7437246 , 0.59285714,\n",
      "       0.67676739]), array([0.84732143, 0.69464286, 0.89821429, 0.69464286, 0.59285714,\n",
      "       0.69464286]), 0.5928571428571429, 0.6156458078077206]\n"
     ]
    }
   ],
   "source": [
    "k=j=0\n",
    "predictions=np.loadtxt('unet_multiClass_predictions_{}_{}.txt'.format(k,j))\n",
    "n=len(predictions[0])\n",
    "confMatrix=np.zeros((n,n))\n",
    "for i in range(len(predictions)):\n",
    "    confMatrix[np.argmax(y_test[i]),np.argmax(predictions[i])]+=1\n",
    "print(confMatrix)\n",
    "print(evaluateMultiClassPredictions(predictions,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPreds=[]\n",
    "predictions=[]\n",
    "trainPreds1=[]\n",
    "predictions1=[]\n",
    "for i in range(len(y_train)):\n",
    "    trainPreds.append(np.loadtxt('unet_segmentation_trainPreds_{}.txt'.format(i)))\n",
    "for i in range(len(y_test)):\n",
    "    predictions.append(np.loadtxt('unet_segmentation_predictions_{}.txt'.format(i)))\n",
    "for i in range(len(y_train)):\n",
    "    trainPreds1.append(np.loadtxt('deep_segmentation_trainPreds_{}.txt'.format(i)))\n",
    "for i in range(len(y_test)):\n",
    "    predictions1.append(np.loadtxt('deep_segmentation_predictions_{}.txt'.format(i)))\n",
    "trainPreds=np.array(trainPreds)\n",
    "predictions=np.array(predictions)\n",
    "trainPreds1=np.array(trainPreds1)\n",
    "predictions1=np.array(predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7241379310344828"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(dicesForLimit(trainPreds1,y_train,0.44)[0])\n",
    "#0.15 0.605, 0.17 0.613, 0.19 0.615, 0.2 0.618, 0.21 0.614, 0.25 0.595, 0.3 0.56\n",
    "#0.2 0.611, 0.25 0.648, 0.3 0.688, 0.35 0.720, 0.4 0.727, 0.43 0.722, 0.44 0.724, 0.45 0.730, 0.46 0.728, 0.5 0.713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=0.2\n",
    "limit1=0.45\n",
    "dices,ious=dicesForLimit(predictions,y_test,limit)\n",
    "dices1,ious1=dicesForLimit(predictions1,y_test,limit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_test(x,y):\n",
    "\n",
    "    x=np.array(x)\n",
    "    y=np.array(y)\n",
    "    f=np.var(x,ddof=1)/np.var(y,ddof=1)\n",
    "    dfn=x.size-1\n",
    "    dfd=y.size-1\n",
    "    p=2*min(1-scipy.stats.f.cdf(f,dfn,dfd),scipy.stats.f.cdf(f,dfn,dfd))\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n",
      "0.403\n",
      "Ttest_relResult(statistic=-4.456370034222895, pvalue=1.4221215233273695e-05)\n",
      "WilcoxonResult(statistic=4428.0, pvalue=6.436702840509736e-06)\n",
      "0.259\n",
      "0.285\n",
      "ShapiroResult(statistic=0.9345442652702332, pvalue=1.3804864806843398e-07)\n",
      "ShapiroResult(statistic=0.9234173893928528, pvalue=1.9048348676165006e-08)\n",
      "0.1822258953078349\n",
      "BartlettResult(statistic=1.7793790527405136, pvalue=0.18222559367108443)\n",
      "LeveneResult(statistic=4.537676241931248, pvalue=0.03379843047166618)\n"
     ]
    }
   ],
   "source": [
    "x=ious\n",
    "y=ious1\n",
    "print(round(np.median(x),3))\n",
    "print(round(np.median(y),3))\n",
    "print(ttest_rel(x,y))\n",
    "print(wilcoxon(x,y))\n",
    "print(round(np.std(x),3))\n",
    "print(round(np.std(y),3))\n",
    "print(shapiro(x))\n",
    "print(shapiro(y))\n",
    "print(f_test(x,y))\n",
    "print(bartlett(x,y))\n",
    "print(levene(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_edges(bool_img):\n",
    "    \"\"\"\n",
    "    Get a list of all edges (where the value changes from True to False) in the 2D boolean image.\n",
    "    The returned array edges has he dimension (n, 2, 2).\n",
    "    Edge i connects the pixels edges[i, 0, :] and edges[i, 1, :].\n",
    "    Note that the indices of a pixel also denote the coordinates of its lower left corner.\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    ii, jj = np.nonzero(bool_img)\n",
    "    for i, j in zip(ii, jj):\n",
    "        # North\n",
    "        if j == bool_img.shape[1]-1 or not bool_img[i, j+1]:\n",
    "            edges.append(np.array([[i, j+1],\n",
    "                                   [i+1, j+1]]))\n",
    "        # East\n",
    "        if i == bool_img.shape[0]-1 or not bool_img[i+1, j]:\n",
    "            edges.append(np.array([[i+1, j],\n",
    "                                   [i+1, j+1]]))\n",
    "        # South\n",
    "        if j == 0 or not bool_img[i, j-1]:\n",
    "            edges.append(np.array([[i, j],\n",
    "                                   [i+1, j]]))\n",
    "        # West\n",
    "        if i == 0 or not bool_img[i-1, j]:\n",
    "            edges.append(np.array([[i, j],\n",
    "                                   [i, j+1]]))\n",
    "\n",
    "    if not edges:\n",
    "        return np.zeros((0, 2, 2))\n",
    "    else:\n",
    "        return np.array(edges)\n",
    "\n",
    "\n",
    "def close_loop_edges(edges):\n",
    "    \"\"\"\n",
    "    Combine the edges defined by 'get_all_edges' to closed loops around objects.\n",
    "    If there are multiple disconnected objects a list of closed loops is returned.\n",
    "    Note that it's expected that all the edges are part of exactly one loop (but not necessarily the same one).\n",
    "    \"\"\"\n",
    "\n",
    "    loop_list = []\n",
    "    while edges.size != 0:\n",
    "\n",
    "        loop = [edges[0, 0], edges[0, 1]]  # Start with first edge\n",
    "        edges = np.delete(edges, 0, axis=0)\n",
    "\n",
    "        while edges.size != 0:\n",
    "            # Get next edge (=edge with common node)\n",
    "            ij = np.nonzero((edges == loop[-1]).all(axis=2))\n",
    "            if ij[0].size > 0:\n",
    "                i = ij[0][0]\n",
    "                j = ij[1][0]\n",
    "            else:\n",
    "                loop.append(loop[0])\n",
    "                # Uncomment to to make the start of the loop invisible when plotting\n",
    "                # loop.append(loop[1])\n",
    "                break\n",
    "\n",
    "            loop.append(edges[i, (j + 1) % 2, :])\n",
    "            edges = np.delete(edges, i, axis=0)\n",
    "\n",
    "        loop_list.append(np.array(loop))\n",
    "\n",
    "    return loop_list\n",
    "\n",
    "\n",
    "def plot_outlines(bool_img, ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    edges = get_all_edges(bool_img=bool_img)\n",
    "    edges = edges - 0.5  # convert indices to coordinates; TODO adjust according to image extent\n",
    "    outlines = close_loop_edges(edges=edges)\n",
    "    cl = LineCollection(outlines, **kwargs)\n",
    "    ax.add_collection(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=8\n",
    "print(i,round(dices1[i],3),round(ious1[i],3))\n",
    "array=cv2.rotate(y_test[i],cv2.cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "array1=cv2.rotate(turnIntoBinaryMask(predictions1[i],limit1),cv2.cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "plt.imshow(array1)\n",
    "plt.imshow(cv2.rotate(x_test[i],cv2.cv2.ROTATE_90_COUNTERCLOCKWISE), cmap='gray')\n",
    "fig=plt.gcf()\n",
    "plot_outlines(array.T, lw=2, color='white')\n",
    "plot_outlines(array1.T, lw=2, color='blue')\n",
    "plt.axis('off')\n",
    "plt.savefig('fig{}.png'.format(i),bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181 16156 17 30\n"
     ]
    }
   ],
   "source": [
    "img_height=128\n",
    "k=8\n",
    "TP=0\n",
    "FN=0\n",
    "FP=0\n",
    "TN=0\n",
    "for i in range(img_height):\n",
    "    for j in range(img_height):\n",
    "        if y_test[k,i,j]>0.5 and predictions1[k,i,j]>limit1:\n",
    "            TP+=1\n",
    "        if y_test[k,i,j]>0.5 and predictions1[k,i,j]<limit1:\n",
    "            FN+=1\n",
    "        if y_test[k,i,j]<0.5 and predictions1[k,i,j]>limit1:\n",
    "            FP+=1\n",
    "        if y_test[k,i,j]<0.5 and predictions1[k,i,j]<limit1:\n",
    "            TN+=1\n",
    "print(TP,TN,FP,FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8850855745721271\n",
      "0.793859649122807\n",
      "0.99713134765625\n"
     ]
    }
   ],
   "source": [
    "print(2*TP/(2*TP+FN+FP))\n",
    "print(TP/(TP+FN+FP))\n",
    "print((TP+TN)/(TP+TN+FN+FP))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
